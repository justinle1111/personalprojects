{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["\n","class NeuralNetwork:\n","    def __init__(self, input_size=784, hidden_layers=[512, 512], output_size=10):\n","        self.input_size = input_size\n","        self.hidden_layers = hidden_layers\n","        self.output_size = output_size\n","        self.weights = []\n","        self.biases = []\n","        self.iterations = 0  # Initialize iterations\n","\n","        # Initializes the first hidden layer\n","        self.weights.append(0.01 * np.random.randn(input_size, hidden_layers[0]))\n","        self.biases.append(np.zeros((1, hidden_layers[0])))\n","\n","        # Initializes additional hidden layers\n","        for i in range(len(hidden_layers) - 1):\n","            self.weights.append(0.01 * np.random.randn(hidden_layers[i], hidden_layers[i + 1]))\n","            self.biases.append(np.zeros((1, hidden_layers[i + 1])))\n","\n","        # Connects last hidden layer to output\n","        self.weights.append(0.01 * np.random.randn(hidden_layers[-1], output_size))\n","        self.biases.append(np.zeros((1, output_size)))\n","\n","    def forward_propagation(self, inputs):\n","        self.outputs = [inputs]\n","        for i in range(len(self.weights)):\n","            self.outputs.append(np.dot(self.outputs[-1], self.weights[i]) + self.biases[i])\n","            if i == (len(self.weights) - 1):\n","                final_output = np.exp(self.outputs[-1] - np.max(self.outputs[-1], axis=1, keepdims=True))\n","                final_output /= np.sum(final_output, axis=1, keepdims=True)\n","                self.outputs.append(final_output)\n","            else:\n","                self.outputs.append(np.maximum(0, self.outputs[-1]))  # ReLU activation\n","        return self.outputs[-1]\n","\n","    @staticmethod\n","    def loss_categorical_cross_entropy(y_pred, y_true):\n","        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n","        loss = -np.sum(y_true * np.log(y_pred), axis=1)\n","        return np.mean(loss)\n","\n","    @staticmethod\n","    def sparse_to_one_hot(sparse_labels, num_classes):\n","        one_hot_encoded = np.zeros((len(sparse_labels), num_classes))\n","        one_hot_encoded[np.arange(len(sparse_labels)), sparse_labels] = 1\n","        return one_hot_encoded\n","\n","    def backwards_propagation(self, y_true):\n","        samples = len(self.outputs[-1])\n","\n","        if len(y_true.shape) == 2:\n","            y_true = np.argmax(y_true, axis=1)\n","\n","        dSoftMaxCrossEntropy = self.outputs[-1].copy()\n","        dSoftMaxCrossEntropy[range(samples), y_true] -= 1\n","        dSoftMaxCrossEntropy /= samples  # Normalize gradient\n","\n","        # Start with final layer gradients\n","        dInputs = dSoftMaxCrossEntropy.copy()\n","        dWeights = np.dot(self.outputs[-3].T, dSoftMaxCrossEntropy)\n","        dBiases = np.sum(dSoftMaxCrossEntropy, axis=0, keepdims=True)\n","\n","        self.gradientWeights = [dWeights]\n","        self.gradientBiases = [dBiases]\n","\n","        i = -3\n","        j = -1\n","        for _ in range(len(self.hidden_layers)):\n","            dInputsRelu = dInputs.copy()\n","            dInputsRelu[self.outputs[i] <= 0] = 0  # ReLU derivative\n","\n","            i -= 1\n","            dInputs = np.dot(dInputsRelu, self.weights[j].T)\n","            dWeights = np.dot(self.outputs[i].T, dInputsRelu)\n","            dBiases = np.sum(dInputsRelu, axis=0, keepdims=True)\n","\n","            self.gradientWeights.insert(0, dWeights)\n","            self.gradientBiases.insert(0, dBiases)\n","\n","            j -= 1\n","\n","    def updateParams(self, lr=0.05, decay=1e-7):\n","        lr = lr * (1. / (1. + decay * self.iterations))\n","\n","        for i in range(len(self.weights)):\n","            assert self.weights[i].shape == self.gradientWeights[i].shape\n","            self.weights[i] -= lr * self.gradientWeights[i]\n","            self.biases[i] -= lr * self.gradientBiases[i]\n","\n","        self.iterations += 1\n","\n","    def SGD(self, lr=0.05, decay=1e-7):\n","        self.updateParams(lr, decay)\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n","[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n"]}],"source":["neuron = NeuralNetwork()\n","print(len(neuron.weights))\n","print(neuron.bias)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":2}
