{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["\n","class NeuralNetwork:\n","    def __init__(self, input_size=784, hidden_layers=[512, 512], output_size=10):\n","        self.input_size = input_size\n","        self.hidden_layers = hidden_layers\n","        self.output_size = output_size\n","        self.weights = []\n","        self.biases = []\n","        self.gradientWeights = []\n","        self.gradientBiases = []\n","        self.iterations = 0  # Initialize iterations\n","\n","        # Initializes the first hidden layer\n","        self.weights.append(0.01 * np.random.randn(input_size, hidden_layers[0]))\n","        self.biases.append(np.zeros((1, hidden_layers[0])))\n","\n","        # Initializes additional hidden layers\n","        for i in range(len(hidden_layers) - 1):\n","            self.weights.append(0.01 * np.random.randn(hidden_layers[i], hidden_layers[i + 1]))\n","            self.biases.append(np.zeros((1, hidden_layers[i + 1])))\n","\n","        # Connects last hidden layer to output\n","        self.weights.append(0.01 * np.random.randn(hidden_layers[-1], output_size))\n","        self.biases.append(np.zeros((1, output_size)))\n","\n","    def forward_propagation(self, inputs):\n","        self.outputs = [inputs]\n","        for i in range(len(self.weights)):\n","            self.outputs.append(np.dot(self.outputs[-1], self.weights[i]) + self.biases[i])\n","            if i == (len(self.weights) - 1):\n","                final_output = np.exp(self.outputs[-1] - np.max(self.outputs[-1], axis=1, keepdims=True))\n","                final_output /= np.sum(final_output, axis=1, keepdims=True)\n","                self.outputs.append(final_output)\n","            else:\n","                self.outputs.append(np.maximum(0, self.outputs[-1]))  # ReLU activation\n","        return self.outputs[-1]\n","\n","    @staticmethod\n","    def loss_categorical_cross_entropy(y_pred, y_true):\n","        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n","        loss = -np.sum(y_true * np.log(y_pred), axis=1)\n","        return np.mean(loss)\n","\n","    @staticmethod\n","    def sparse_to_one_hot(sparse_labels, num_classes):\n","        one_hot_encoded = np.zeros((len(sparse_labels), num_classes))\n","        one_hot_encoded[np.arange(len(sparse_labels)), sparse_labels] = 1\n","        return one_hot_encoded\n","\n","    def backwards_propagation(self, y_true):\n","        samples = len(self.outputs[-1])\n","\n","        if len(y_true.shape) == 2:\n","            y_true = np.argmax(y_true, axis=1)\n","\n","        dSoftMaxCrossEntropy = self.outputs[-1].copy()\n","        dSoftMaxCrossEntropy[range(samples), y_true] -= 1\n","        dSoftMaxCrossEntropy /= samples  # Normalize gradient\n","\n","        # Start with final layer gradients\n","        dInputs = np.dot(dSoftMaxCrossEntropy.copy(), self.weights[-1].T)\n","        dWeights = np.dot(self.outputs[-3].T, dSoftMaxCrossEntropy.copy())\n","        dBiases = np.sum(dSoftMaxCrossEntropy.copy(), axis=0, keepdims=True)\n","\n","        self.gradientWeights = [dWeights] + self.gradientWeights\n","        self.gradientBiases = [dBiases] + self.gradientBiases\n","\n","        for layer in range(len(self.hidden_layers), 0, -1):  # Reverse loop from last hidden layer to first\n","            dInputsRelu = dInputs.copy()\n","            dInputsRelu[self.outputs[layer] <= 0] = 0  # Apply ReLU derivative\n","\n","            dInputs = np.dot(dInputsRelu, self.weights[layer].T)  # Backprop error to previous layer\n","            dWeights = np.dot(self.outputs[layer - 1].T, dInputsRelu)  # Compute weight gradients\n","            dBiases = np.sum(dInputsRelu, axis=0, keepdims=True)  # Compute bias gradients\n","\n","            self.gradientWeights.insert(0, dWeights)\n","            self.gradientBiases.insert(0, dBiases)\n","\n","        \n","    def SGD(self, lr=0.05, decay=1e-7):\n","        lr = lr * (1. / (1. + decay * self.iterations))\n","\n","        for i in range(len(self.weights)):\n","            assert self.weights[i].shape == self.gradientWeights[i].shape\n","            self.weights[i] -= lr * self.gradientWeights[i]\n","            self.biases[i] -= lr * self.gradientBiases[i]\n","\n","        self.iterations += 1\n","\n","    def train(self, X_train, y_train, epochs=10, batch_size=32, lr=0.05):\n","        num_samples = X_train.shape[0]\n","\n","        for epoch in range(epochs):\n","            shuffled_indices = np.random.permutation(num_samples)\n","            X_train, y_train = X_train[shuffled_indices], y_train[shuffled_indices]\n","\n","            for i in range(0, num_samples, batch_size):\n","                X_batch = X_train[i:i + batch_size]\n","                y_batch = y_train[i:i + batch_size]\n","\n","                # Forward pass\n","                y_pred = self.forward_propagation(X_batch)\n","\n","                # Convert labels to one-hot encoding\n","                y_batch_one_hot = self.sparse_to_one_hot(y_batch, self.output_size)\n","\n","                # Compute loss\n","                loss = self.loss_categorical_cross_entropy(y_pred, y_batch_one_hot)\n","\n","                # Backward pass\n","                self.backwards_propagation(y_batch_one_hot)\n","\n","                # Update weights\n","                self.SGD(lr)\n","\n","            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"shapes (64,512) and (10,512) not aligned: 512 (dim 1) != 10 (dim 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m y_test_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(num_classes)[y_test]\n\u001b[1;32m     15\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call manually\u001b[39;00m\n","Cell \u001b[0;32mIn[31], line 110\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X_train, y_train, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_categorical_cross_entropy(y_pred, y_batch_one_hot)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackwards_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch_one_hot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSGD(lr)\n","Cell \u001b[0;32mIn[31], line 71\u001b[0m, in \u001b[0;36mNeuralNetwork.backwards_propagation\u001b[0;34m(self, y_true)\u001b[0m\n\u001b[1;32m     68\u001b[0m dInputsRelu \u001b[38;5;241m=\u001b[39m dInputs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     69\u001b[0m dInputsRelu[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[layer] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Apply ReLU derivative\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m dInputs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdInputsRelu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backprop error to previous layer\u001b[39;00m\n\u001b[1;32m     72\u001b[0m dWeights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs[layer \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT, dInputsRelu)  \u001b[38;5;66;03m# Compute weight gradients\u001b[39;00m\n\u001b[1;32m     73\u001b[0m dBiases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dInputsRelu, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Compute bias gradients\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: shapes (64,512) and (10,512) not aligned: 512 (dim 1) != 10 (dim 0)"]}],"source":["from tensorflow.keras.datasets import mnist\n","\n","# Load dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Normalize pixel values to [0, 1]\n","x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n","x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n","\n","# Convert labels to one-hot encoding\n","num_classes = 10\n","y_train_one_hot = np.eye(num_classes)[y_train]\n","y_test_one_hot = np.eye(num_classes)[y_test]\n","\n","nn = NeuralNetwork()\n","nn.train(x_train, y_train, epochs=10, batch_size=64, lr=0.01)  # Call manually\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":2}
